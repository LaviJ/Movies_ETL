{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "from config import db_password\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Add the clean movie function that takes in the argument, \"movie\".\n",
    "def clean_movie(movie):\n",
    "    movie = dict(movie) #create a non-destructive copy\n",
    "    alt_titles = {}\n",
    "    # combine alternate titles into one list\n",
    "    for key in ['Also known as','Arabic','Cantonese','Chinese','French',\n",
    "                'Hangul','Hebrew','Hepburn','Japanese','Literally',\n",
    "                'Mandarin','McCune–Reischauer','Original title','Polish',\n",
    "                'Revised Romanization','Romanized','Russian',\n",
    "                'Simplified','Traditional','Yiddish']:\n",
    "        if key in movie:\n",
    "            alt_titles[key] = movie[key]\n",
    "            movie.pop(key)\n",
    "    if len(alt_titles) > 0:\n",
    "        movie['alt_titles'] = alt_titles\n",
    "\n",
    "\n",
    "# merge column names... new function is enclosed within the clean_movie function that we created earlier:\n",
    "    def change_column_name(old_name, new_name):\n",
    "        if old_name in movie:\n",
    "            movie[new_name] = movie.pop(old_name)\n",
    "# To change every instance where the key is \"Directed by\" to the new key \"Director,\" write the following inside clean_movie():           \n",
    "    change_column_name('Directed by', 'Director')\n",
    "    \n",
    "# To change other columns :\n",
    "    change_column_name('Adaptation by', 'Writer(s)')\n",
    "    change_column_name('Country of origin', 'Country')\n",
    "    change_column_name('Directed by', 'Director')\n",
    "    change_column_name('Distributed by', 'Distributor')\n",
    "    change_column_name('Edited by', 'Editor(s)')\n",
    "    change_column_name('Length', 'Running time')\n",
    "    change_column_name('Original release', 'Release date')\n",
    "    change_column_name('Music by', 'Composer(s)')\n",
    "    change_column_name('Produced by', 'Producer(s)')\n",
    "    change_column_name('Producer', 'Producer(s)')\n",
    "    change_column_name('Productioncompanies ', 'Production company(s)')\n",
    "    change_column_name('Productioncompany ', 'Production company(s)')\n",
    "    change_column_name('Released', 'Release Date')\n",
    "    change_column_name('Release Date', 'Release date')\n",
    "    change_column_name('Screen story by', 'Writer(s)')\n",
    "    change_column_name('Screenplay by', 'Writer(s)')\n",
    "    change_column_name('Story by', 'Writer(s)')\n",
    "    change_column_name('Theme music composer', 'Composer(s)')\n",
    "    change_column_name('Written by', 'Writer(s)')    \n",
    "    \n",
    "    return movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Add the function that takes in three arguments;\n",
    "# Wikipedia data, Kaggle metadata, and MovieLens rating data (from Kaggle)\n",
    "# def function_name():\n",
    "def extract_transform_load(wiki_file,kaggle_file,ratings_file):\n",
    "    \n",
    "    # Read in the kaggle metadata and MovieLens ratings CSV files as Pandas DataFrames.\n",
    "    ratings = pd.read_csv(ratings_file, low_memory=False)\n",
    "    kaggle_metadata = pd.read_csv(kaggle_file, low_memory=False)\n",
    "    \n",
    "    # Open the read the Wikipedia data JSON file.\n",
    "    with open(wiki_file, mode='r') as file:\n",
    "        json_file = json.load(file)        \n",
    "      \n",
    "    # Deliverable 2\n",
    "    \n",
    "    # 3. write a list comprehension that filters out TV shows from the wiki_movies_raw file\n",
    "    wiki_movies = [movie for movie in json_file\n",
    "               if ('Director' in movie or 'Directed by' in movie)\n",
    "                   and ('imdb_link' in movie)\n",
    "                   and ('No. of episodes' not in movie)]\n",
    "    \n",
    "    clean_movies = [clean_movie(movie) for movie in wiki_movies]\n",
    "        \n",
    "    # 5. read in the cleaned movies list from Step 4 as a DataFrame.\n",
    "    wiki_movies_df = pd.DataFrame(clean_movies)   \n",
    "    \n",
    "    \n",
    "    \n",
    "    # 6, write a try-except block that will catch errors while extracting the IMDb IDs with a regular expression string and dropping any imdb_id duplicates\n",
    "    try:\n",
    "        wiki_movies_df['imdb_id'] = wiki_movies_df['imdb_link'].str.extract(r'(tt\\d{7})')\n",
    "    except exception as e:\n",
    "        print(f\"Error while retrieving Imdb_ID {e}.\")\n",
    "        \n",
    "    # 7. write a list comprehension to keep the columns that have non-null values\n",
    "    wiki_movies_nonnull_columns = []\n",
    "    wiki_movies_df_columns = wiki_movies_df.columns.tolist()\n",
    "    for column in wiki_movies_df_columns:\n",
    "        if (wiki_movies_df[column].isnull().sum() < len(wiki_movies_df) * 0.9):\n",
    "            wiki_movies_nonnull_columns.append(column)\n",
    "    \n",
    "    wiki_movies_df = wiki_movies_df[wiki_movies_nonnull_columns]\n",
    "    \n",
    "    wiki_movies_df.drop_duplicates(subset='imdb_id', inplace=True)\n",
    "\n",
    "    \n",
    "    # 8. Create a variable that will hold the non-null values from the “Box office” column.\n",
    "    box_office = wiki_movies_df[\"Box office\"].dropna()\n",
    "    \n",
    "    # 9. Convert the box office data created in Step 8 to string values using the lambda and join functions.\n",
    "    box_office = box_office.apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "\n",
    "    # 10. Write a regular expression to match the six elements of \"form_one\" of the box office data.\n",
    "    form_one = r'\\$\\s*\\d+\\.?\\d*\\s*[mb]illi?on'\n",
    "   \n",
    "    # 11. Write a regular expression to match the three elements of \"form_two\" of the box office data.\n",
    "    form_two = r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)'\n",
    "\n",
    "    # 12. Add the parse_dollars function.\n",
    "    def parse_dollars(s):\n",
    "    # if s is not a string, return NaN\n",
    "        if type(s) != str:\n",
    "            return np.nan\n",
    "\n",
    "        # if input is of the form $###.# million\n",
    "        if re.match(r'\\$\\s*\\d+\\.?\\d*\\s*milli?on', s, flags=re.IGNORECASE):\n",
    "\n",
    "            # remove dollar sign and \" million\"\n",
    "            s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "\n",
    "            # convert to float and multiply by a million\n",
    "            value = float(s) * 10**6\n",
    "\n",
    "            # return value\n",
    "            return value\n",
    "\n",
    "        # if input is of the form $###.# billion\n",
    "        elif re.match(r'\\$\\s*\\d+\\.?\\d*\\s*billi?on', s, flags=re.IGNORECASE):\n",
    "\n",
    "            # remove dollar sign and \" billion\"\n",
    "            s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "\n",
    "            # convert to float and multiply by a billion\n",
    "            value = float(s) * 10**9\n",
    "\n",
    "            # return value\n",
    "            return value\n",
    "\n",
    "        # if input is of the form $###,###,###\n",
    "        elif re.match(r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)', s, flags=re.IGNORECASE):\n",
    "\n",
    "            # remove dollar sign and commas\n",
    "            s = re.sub('\\$|,','', s)\n",
    "\n",
    "            # convert to float\n",
    "            value = float(s)\n",
    "\n",
    "            # return value\n",
    "            return value\n",
    "\n",
    "        # otherwise, return NaN\n",
    "        else:\n",
    "            return np.nan\n",
    "    \n",
    "               \n",
    "    # 13. Clean the box office column in the wiki_movies_df DataFrame.\n",
    "    wiki_movies_df['box_office'] = box_office.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "    wiki_movies_df.drop(\"Box office\", inplace=True, axis=1)\n",
    "    \n",
    "    # 14. Clean the budget column in the wiki_movies_df DataFrame.\n",
    "    budget = wiki_movies_df[\"Budget\"].dropna()\n",
    "    \n",
    "    budget = budget.map(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    \n",
    "    # remove any values between a dollar sign and a hyphen (for budgets given in ranges):\n",
    "    budget = budget.str.replace(r'\\$.*[-—–](?![a-z])', '$', regex=True)\n",
    "    budget = budget.str.replace(r'\\[\\d+\\]\\s*', '')   \n",
    "    \n",
    "    # parse the budget values\n",
    "    wiki_movies_df['budget'] = budget.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "        \n",
    "    # 15. Clean the release date column in the wiki_movies_df DataFrame.\n",
    "    release_date = wiki_movies_df[\"Release date\"].dropna().apply(lambda x: ' '.join(x) if type(x)==list else x)\n",
    "    \n",
    "    # parse those forms\n",
    "    date_form_one = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s[123]?\\d,\\s\\d{4}'\n",
    "    date_form_two = r'\\d{4}.[01]\\d.[0123]\\d'\n",
    "    date_form_three = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{4}'\n",
    "    date_form_four = r'\\d{4}'\n",
    "    \n",
    "    # use the built-in to_datetime() method in Pandas, set the infer_datetime_format option to True\n",
    "    wiki_movies_df['release_date'] = pd.to_datetime(release_date.str.extract(f'({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})')[0], infer_datetime_format=True)\n",
    "        \n",
    "    # 16. Clean the running time column in the wiki_movies_df DataFrame.\n",
    "    running_time = wiki_movies_df[\"Running time\"].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    \n",
    "    # capture some more of these by relaxing the condition that the pattern has to start with at the beginning of the string\n",
    "    # extract digits, and we want to allow for both possible patterns. Therefore, we'll add capture groups around the \\d instances as well as add an alternating character\n",
    "    running_time_extract = running_time.str.extract(r'(\\d+)\\s*ho?u?r?s?\\s*(\\d*)|(\\d+)\\s*m')\n",
    "    \n",
    "    # this new DataFrame is all strings, we'll need to convert them to numeric values. Because we may have captured empty strings, we'll use the to_numeric() method and set the errors argument to 'coerce'. Coercing the errors will turn the empty strings into Not a Number (NaN), then we can use fillna() to change all the NaNs to zeros.\n",
    "    running_time_extract = running_time_extract.apply(lambda col: pd.to_numeric(col, errors='coerce')).fillna(0)\n",
    "    \n",
    "    # apply a function that will convert the hour capture groups and minute capture groups to minutes if the pure minutes capture group is zero, and save the output to wiki_movies_df\n",
    "    wiki_movies_df['running_time'] = running_time_extract.apply(lambda row: row[0]*60 + row[1] if row[2] == 0 else row[2], axis=1)\n",
    "    wiki_movies_df.drop(\"Running time\", inplace=True, axis=1)\n",
    "    \n",
    "    # Deliverable 3 :\n",
    "    # 2. Clean the Kaggle metadata.\n",
    "    # keep rows where the adult column is False, and then drop the adult column\n",
    "    kaggle_metadata = kaggle_metadata[kaggle_metadata['adult'] == 'False'].drop('adult',axis='columns')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # For the numeric columns, use the to_numeric() method, make sure the errors= argument is set to 'raise'\n",
    "    kaggle_metadata['budget'] = kaggle_metadata['budget'].astype(int)\n",
    "    kaggle_metadata['id'] = pd.to_numeric(kaggle_metadata['id'], errors='raise')\n",
    "    kaggle_metadata['popularity'] = pd.to_numeric(kaggle_metadata['popularity'], errors='raise')\n",
    "    \n",
    "    # convert release_date to datetime using built-in function for that as well: to_datetime()\n",
    "    kaggle_metadata['release_date'] = pd.to_datetime(kaggle_metadata['release_date']) \n",
    "    \n",
    "    \n",
    "\n",
    "    # 3. Merged the two DataFrames into the movies DataFrame.\n",
    "    movies_df = pd.merge(wiki_movies_df, kaggle_metadata, on='imdb_id', suffixes=['_wiki','_kaggle'])\n",
    "    \n",
    "    \n",
    "\n",
    "    # 4. Drop unnecessary columns from the merged DataFrame.\n",
    "    # drop the title_wiki, release_date_wiki, Language, and Production company(s) columns\n",
    "    movies_df.drop(columns=['title_wiki','release_date_wiki','Language','Production company(s)'], inplace=True)\n",
    "    \n",
    "    \n",
    "\n",
    "    # 5. Add in the function to fill in the missing Kaggle data.    \n",
    "    def fill_missing_kaggle_data(df, kaggle_column, wiki_column):\n",
    "        df[kaggle_column] = df.apply(\n",
    "            lambda row: row[wiki_column] if row[kaggle_column] == 0 else row[kaggle_column]\n",
    "            , axis=1)\n",
    "        df.drop(columns=wiki_column, inplace=True)   \n",
    "\n",
    "\n",
    "    # 6. Call the function in Step 5 with the DataFrame and columns as the arguments.\n",
    "    fill_missing_kaggle_data(movies_df, 'runtime', 'running_time')\n",
    "    fill_missing_kaggle_data(movies_df, 'budget_kaggle', 'budget_wiki')\n",
    "    fill_missing_kaggle_data(movies_df, 'revenue', 'box_office')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 7. Filter the movies DataFrame for specific columns.\n",
    "    # check that there aren't any columns with only one value, since that doesn't really provide any information\n",
    "    for col in movies_df.columns:\n",
    "        lists_to_tuples = lambda x: tuple(x) if type(x) == list else x\n",
    "        value_counts = movies_df[col].apply(lists_to_tuples).value_counts(dropna=False)\n",
    "        num_values = len(value_counts)\n",
    "        if num_values == 1:\n",
    "            movies_df.drop(col, inplace=True, axis=1)\n",
    "            \n",
    "    \n",
    "\n",
    "    # 8. Rename the columns in the movies DataFrame.\n",
    "    # to reorder the columns\n",
    "    movies_df = movies_df.loc[:, ['imdb_id','id','title_kaggle','original_title','tagline','belongs_to_collection','url','imdb_link',\n",
    "                       'runtime','budget_kaggle','revenue','release_date_kaggle','popularity','vote_average','vote_count',\n",
    "                       'genres','original_language','overview','spoken_languages','Country',\n",
    "                       'production_companies','production_countries','Distributor',\n",
    "                       'Producer(s)','Director','Starring','Cinematography','Editor(s)','Writer(s)','Composer(s)','Based on'\n",
    "                      ]]\n",
    "                         \n",
    "    # to rename the columns to be consistent\n",
    "    movies_df.rename({'id':'kaggle_id',\n",
    "                  'title_kaggle':'title',\n",
    "                  'url':'wikipedia_url',\n",
    "                  'budget_kaggle':'budget',\n",
    "                  'release_date_kaggle':'release_date',\n",
    "                  'Country':'country',\n",
    "                  'Distributor':'distributor',\n",
    "                  'Producer(s)':'producers',\n",
    "                  'Director':'director',\n",
    "                  'Starring':'starring',\n",
    "                  'Cinematography':'cinematography',\n",
    "                  'Editor(s)':'editors',\n",
    "                  'Writer(s)':'writers',\n",
    "                  'Composer(s)':'composers',\n",
    "                  'Based on':'based_on'\n",
    "                 }, axis='columns', inplace=True)\n",
    "                \n",
    "   \n",
    "    \n",
    "    # 9. Transform and merge the ratings DataFrame.\n",
    "    # rename the \"userId\" column to \"count.\"\n",
    "    # pivot this data so that movieId is the index, the columns will be all the rating values, and the rows will be the counts for each rating value.\n",
    "    rating_counts = ratings.groupby(['movieId','rating'], as_index=False).count() \\\n",
    "                .rename({'userId':'count'}, axis=1) \\\n",
    "                .pivot(index='movieId',columns='rating', values='count')\n",
    "                         \n",
    "    # rename the columns so they're easier to understand. We'll prepend rating_ to each column with a list comprehension\n",
    "    rating_counts.columns = ['rating_' + str(col) for col in rating_counts.columns]\n",
    "    \n",
    "   \n",
    "                         \n",
    "    # use a left merge, since we want to keep everything in movies_df\n",
    "    movies_with_ratings_df = pd.merge(movies_df, rating_counts, left_on='kaggle_id', right_index=True, how='left')\n",
    "    \n",
    "    \n",
    "    # missing values to be filled with zeros\n",
    "    movies_with_ratings_df[rating_counts.columns] = movies_with_ratings_df[rating_counts.columns].fillna(0)\n",
    "    \n",
    "    db_string = f\"postgresql://postgres:{db_password}@127.0.0.1:5433/movie_data\"\n",
    "    \n",
    "    # Create the database engine\n",
    "    engine = create_engine(db_string)\n",
    "    \n",
    "    movies_df.to_sql(name='movies', con=engine, if_exists='replace')\n",
    "    \n",
    "    #reimport the CSV using the chunksize= parameter in read_csv()\n",
    "    #create a variable for the number of rows imported\n",
    "    rows_imported = 0\n",
    "    # get the start_time from time.time()\n",
    "    start_time = time.time()\n",
    "    for data in pd.read_csv(ratings_file, chunksize=1000000):\n",
    "\n",
    "        # print out the range of rows that are being imported\n",
    "        print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "        data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "        # increment the number of rows imported by the chunksize\n",
    "        rows_imported += len(data)\n",
    "\n",
    "        # add elapsed time to final print out\n",
    "        print(f'Done. {time.time() - start_time} total seconds elapsed')\n",
    "        \n",
    "    return 'Done'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Create the path to your file directory and variables for the three files.\n",
    "file_dir = 'C:/UC_Berkeley/Analysis_Projects_Class_Folder/Module8_ETL/Data_for_Movies_ETL'\n",
    "# The Wikipedia data\n",
    "wiki_file = f'{file_dir}/wikipedia-movies.json'\n",
    "# The Kaggle metadata\n",
    "kaggle_file = f'{file_dir}/movies_metadata.csv'\n",
    "# The MovieLens rating data.\n",
    "ratings_file = f'{file_dir}/ratings.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-910d3b987dd8>:117: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  budget = budget.str.replace(r'\\[\\d+\\]\\s*', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing rows 0 to 1000000...Done. 49.482200384140015 total seconds elapsed\n",
      "importing rows 1000000 to 2000000...Done. 92.17748618125916 total seconds elapsed\n",
      "importing rows 2000000 to 3000000...Done. 136.02251482009888 total seconds elapsed\n",
      "importing rows 3000000 to 4000000...Done. 170.89365029335022 total seconds elapsed\n",
      "importing rows 4000000 to 5000000...Done. 204.6585772037506 total seconds elapsed\n",
      "importing rows 5000000 to 6000000...Done. 243.78049778938293 total seconds elapsed\n",
      "importing rows 6000000 to 7000000...Done. 286.24219822883606 total seconds elapsed\n",
      "importing rows 7000000 to 8000000...Done. 335.50787568092346 total seconds elapsed\n",
      "importing rows 8000000 to 9000000..."
     ]
    }
   ],
   "source": [
    "# 11. Set the three variables equal to the function created in D1.\n",
    "Status = extract_transform_load(wiki_file, kaggle_file, ratings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Status)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
